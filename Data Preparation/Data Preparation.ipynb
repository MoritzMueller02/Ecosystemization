{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c010001e",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "- Downloading the Features of 200 manually coded companies from the S&P1500\n",
    "- The tickers are already coded into one of the four classes \n",
    "    - 1 = Orchestrator Core\n",
    "    - 2 = Orchestrator Periph.\n",
    "    - 3 = Complementor Core\n",
    "    - 4 = Complementor Peripheral\n",
    "    \n",
    "- We import the necessary libraries and read the Excel file containing the company list with their corresponding tickers and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd6b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://financialmodelingprep.com/api\"\n",
    "API_KEY = \"ieZWryBMhiEhowJQXvvJBSo8rcJfvMVi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843af593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Ticker    200 non-null    object\n",
      " 1   Label     200 non-null    int64 \n",
      " 2   Partners  200 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tickers_df = pd.read_excel(\"CompanyList_Coded.xlsx\")\n",
    "tickers_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9785ca",
   "metadata": {},
   "source": [
    "## Importing the Balance Sheet Data, Income Statement and CashFlowStatement Data\n",
    "\n",
    "Next, we import the Balance Sheet, Income Statement, and Cash Flow Statement data for each company using their tickers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c0d91",
   "metadata": {},
   "source": [
    "# Function for Importing Data using the V3 Version of the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1df48e00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching balance-sheet-statement data: 176 ticker [02:11,  1.33 ticker/s, Ticker=STZ]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Assuming tickers_df is your DataFrame containing tickers and labels\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m balancesheet_df \u001b[38;5;241m=\u001b[39m fetch_and_concat_data(tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m], tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartners\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalance-sheet-statement\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m,API_KEY)\n\u001b[0;32m     30\u001b[0m incomestatement_df \u001b[38;5;241m=\u001b[39m fetch_and_concat_data(tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m], tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartners\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome-statement\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, API_KEY)\n\u001b[0;32m     31\u001b[0m cashflowstatement_df \u001b[38;5;241m=\u001b[39m fetch_and_concat_data(tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m], tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],tickers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartners\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcash-flow-statement\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, API_KEY)\n",
      "Cell \u001b[1;32mIn[46], line 14\u001b[0m, in \u001b[0;36mfetch_and_concat_data\u001b[1;34m(tickers, labels, partner, statement_type, base_url, version, API_KEY)\u001b[0m\n\u001b[0;32m     12\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m: ticker})\n\u001b[0;32m     13\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatement_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?apikey=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1056\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1057\u001b[0m         (\n\u001b[0;32m   1058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1064\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    365\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m     sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_and_concat_data(tickers, labels, partner, statement_type, base_url, version, API_KEY):\n",
    "    dfs = []  # List to store DataFrames for each company\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(zip(tickers, labels, partner), desc=f\"Fetching {statement_type} data\", unit=\" ticker\")\n",
    "    \n",
    "    for ticker, label, partner in progress_bar:\n",
    "        progress_bar.set_postfix({\"Ticker\": ticker})\n",
    "        url = f\"{base_url}/{version}/{statement_type}/{ticker}?apikey={API_KEY}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Ticker'] = ticker  # Adding a column for ticker symbol\n",
    "        df['Label'] = label \n",
    "        df['Partner'] = partner# Adding a column for label\n",
    "        dfs.append(df)\n",
    "        \n",
    "            \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.head()\n",
    "    return final_df\n",
    "\n",
    "# Assuming tickers_df is your DataFrame containing tickers and labels\n",
    "balancesheet_df = fetch_and_concat_data(tickers_df[\"Ticker\"], tickers_df[\"Label\"],tickers_df[\"Partners\"], \"balance-sheet-statement\", base_url, 'v3',API_KEY)\n",
    "incomestatement_df = fetch_and_concat_data(tickers_df[\"Ticker\"], tickers_df[\"Label\"],tickers_df[\"Partners\"], \"income-statement\", base_url,'v3', API_KEY)\n",
    "cashflowstatement_df = fetch_and_concat_data(tickers_df[\"Ticker\"], tickers_df[\"Label\"],tickers_df[\"Partners\"], \"cash-flow-statement\", base_url,'v3', API_KEY)\n",
    "keymetrics_df = fetch_and_concat_data(tickers_df[\"Ticker\"], tickers_df[\"Label\"],tickers_df[\"Partners\"], 'key-metrics', base_url,'v3', API_KEY)\n",
    "# analyststockrec_df = fetch_and_concat_data(tickers_df[\"Ticker\"], tickers_df[\"Label\"], 'analyst-stock-recommendations', base_url,'v3', API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56207fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check which symbols are missing in balancesheet_df.symbol\n",
    "# missing_symbols = tickers_df[~tickers_df['Ticker'].isin(employeecount_df['symbol'])]\n",
    "\n",
    "# Print the missing symbols\n",
    "# print(\"Symbols missing in balancesheet_df:\")\n",
    "# print(missing_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming dfs is a list of DataFrames: balancesheet_df, incomestatement_df, cashflowstatement_df, keymetrics_df\n",
    "dfs = [balancesheet_df, incomestatement_df, cashflowstatement_df, keymetrics_df]\n",
    "\n",
    "# Process each DataFrame in the list\n",
    "for df in dfs:\n",
    "    # Check if 'date' column exists to extract 'calendarYear'\n",
    "    if 'calendarYear' in df.columns:\n",
    "        df['calendarYear'] = pd.DatetimeIndex(df['calendarYear']).year  # Extract year from date\n",
    "        # Filter rows where 'calendarYear' is between 2007 and 2023\n",
    "        df = df[df['calendarYear'].between(2007, 2023)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "balancesheet_df = balancesheet_df[balancesheet_df['calendarYear'].between(2007, 2024)]\n",
    "incomestatement_df = incomestatement_df[incomestatement_df['calendarYear'].between(2007, 2024)]\n",
    "cashflowstatement_df = cashflowstatement_df[cashflowstatement_df['calendarYear'].between(2007, 2024)]\n",
    "keymetrics_df = keymetrics_df[keymetrics_df['calendarYear'].between(2007, 2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "balancesheet_df.calendarYear.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be59310",
   "metadata": {},
   "source": [
    "# Import Data from API using V4\n",
    "\n",
    "Some features have a different link structure, therefore i used two different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b58081f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching historical/employee_count data: 200ticker [02:36,  1.28ticker/s, Ticker=DLTR] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def fetch_and_concat_data_v4(tickers, labels, partner,statement_type, base_url, version, API_KEY):\n",
    "    dfs = []  # List to store DataFrames for each company\n",
    "    years = range(2023, 2006, -1)  # Define range of years for synthetic data\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(zip(tickers, labels, partner), desc=f\"Fetching {statement_type} data\", unit=\"ticker\")\n",
    "    \n",
    "    for ticker, label, partner in progress_bar:\n",
    "        progress_bar.set_postfix({\"Ticker\": ticker})\n",
    "        url = f\"{base_url}/{version}/{statement_type}?symbol={ticker}&apikey={API_KEY}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Ticker'] = ticker  # Adding a column for ticker symbol\n",
    "        df['Label'] = label\n",
    "        df['Partners'] = partner                       # Adding a column for label\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "employeecount_df = fetch_and_concat_data_v4(tickers_df[\"Ticker\"], tickers_df[\"Label\"],tickers_df[\"Partners\"] ,'historical/employee_count', base_url, 'v4', API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8db3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_missing_values(employeecount_df, tickers_df):\n",
    "    # Loop through each ticker in tickers_df\n",
    "    for index, row in tickers_df.iterrows():\n",
    "        symbol = row['Ticker']\n",
    "        label = row['Label']\n",
    "\n",
    "        # Check if the symbol is in employeecount_df\n",
    "        if symbol not in employeecount_df['symbol'].values:\n",
    "            years = range(2023, 2006, -1)  # Define range of years for synthetic data\n",
    "            synthetic_data = {\n",
    "                'symbol': [symbol] * len(years),\n",
    "                'cik': [np.nan] * len(years),\n",
    "                'acceptanceTime': [np.nan] * len(years),\n",
    "                'periodOfReport': [f\"{year}-12-31\" for year in years],  # Example period of report dates\n",
    "                'companyName': [label] * len(years),\n",
    "                'formType': [np.nan] * len(years),\n",
    "                'filingDate': [np.nan] * len(years),\n",
    "                'employeeCount': [np.nan] * len(years),\n",
    "                'source': [np.nan] * len(years),\n",
    "                'Label': [label] * len(years)\n",
    "            }\n",
    "            df = pd.DataFrame(synthetic_data)\n",
    "            employeecount_df = pd.concat([employeecount_df, df], ignore_index=True)\n",
    "        \n",
    "    return employeecount_df\n",
    "\n",
    "\n",
    "employeecount_df = fill_missing_values(employeecount_df, tickers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "132f41bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>cik</th>\n",
       "      <th>acceptanceTime</th>\n",
       "      <th>periodOfReport</th>\n",
       "      <th>companyName</th>\n",
       "      <th>formType</th>\n",
       "      <th>filingDate</th>\n",
       "      <th>employeeCount</th>\n",
       "      <th>source</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Label</th>\n",
       "      <th>Partners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>2023-11-02 18:08:27</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>161000.0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>2022-10-27 18:01:14</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2022-10-28</td>\n",
       "      <td>164000.0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>2021-10-28 18:04:28</td>\n",
       "      <td>2021-09-25</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>154000.0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>2020-10-29 18:06:25</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>147000.0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>2019-10-30 18:12:36</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>137000.0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>TKO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>TKO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>TKO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>TKO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>TKO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4350 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol         cik       acceptanceTime periodOfReport companyName  \\\n",
       "0      AAPL  0000320193  2023-11-02 18:08:27     2023-09-30  Apple Inc.   \n",
       "1      AAPL  0000320193  2022-10-27 18:01:14     2022-09-24  Apple Inc.   \n",
       "2      AAPL  0000320193  2021-10-28 18:04:28     2021-09-25  Apple Inc.   \n",
       "3      AAPL  0000320193  2020-10-29 18:06:25     2020-09-26  Apple Inc.   \n",
       "4      AAPL  0000320193  2019-10-30 18:12:36     2019-09-28  Apple Inc.   \n",
       "...     ...         ...                  ...            ...         ...   \n",
       "4345    TKO         NaN                  NaN     2011-12-31           2   \n",
       "4346    TKO         NaN                  NaN     2010-12-31           2   \n",
       "4347    TKO         NaN                  NaN     2009-12-31           2   \n",
       "4348    TKO         NaN                  NaN     2008-12-31           2   \n",
       "4349    TKO         NaN                  NaN     2007-12-31           2   \n",
       "\n",
       "     formType  filingDate  employeeCount  \\\n",
       "0        10-K  2023-11-03       161000.0   \n",
       "1        10-K  2022-10-28       164000.0   \n",
       "2        10-K  2021-10-29       154000.0   \n",
       "3        10-K  2020-10-30       147000.0   \n",
       "4        10-K  2019-10-31       137000.0   \n",
       "...       ...         ...            ...   \n",
       "4345      NaN         NaN            NaN   \n",
       "4346      NaN         NaN            NaN   \n",
       "4347      NaN         NaN            NaN   \n",
       "4348      NaN         NaN            NaN   \n",
       "4349      NaN         NaN            NaN   \n",
       "\n",
       "                                                 source Ticker  Label  \\\n",
       "0     https://www.sec.gov/Archives/edgar/data/320193...   AAPL      1   \n",
       "1     https://www.sec.gov/Archives/edgar/data/320193...   AAPL      1   \n",
       "2     https://www.sec.gov/Archives/edgar/data/320193...   AAPL      1   \n",
       "3     https://www.sec.gov/Archives/edgar/data/320193...   AAPL      1   \n",
       "4     https://www.sec.gov/Archives/edgar/data/320193...   AAPL      1   \n",
       "...                                                 ...    ...    ...   \n",
       "4345                                                NaN    NaN      2   \n",
       "4346                                                NaN    NaN      2   \n",
       "4347                                                NaN    NaN      2   \n",
       "4348                                                NaN    NaN      2   \n",
       "4349                                                NaN    NaN      2   \n",
       "\n",
       "      Partners  \n",
       "0        373.0  \n",
       "1        373.0  \n",
       "2        373.0  \n",
       "3        373.0  \n",
       "4        373.0  \n",
       "...        ...  \n",
       "4345       NaN  \n",
       "4346       NaN  \n",
       "4347       NaN  \n",
       "4348       NaN  \n",
       "4349       NaN  \n",
       "\n",
       "[4350 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeecount_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ddd9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeecount_df = employeecount_df.rename(columns={\"periodOfReport\": \"calendarYear\"}) \n",
    "employeecount_df[\"calendarYear\"] = pd.to_datetime(employeecount_df[\"calendarYear\"]).dt.year\n",
    "# employeecount_df = employeecount_df[employeecount_df['calendarYear'].between(2007, 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a26d792",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.dtype[int64]' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m employeecount_df\u001b[38;5;241m.\u001b[39mcalendarYear\u001b[38;5;241m.\u001b[39mdtypes()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.dtype[int64]' object is not callable"
     ]
    }
   ],
   "source": [
    "employeecount_df.calendarYear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61af6f1",
   "metadata": {},
   "source": [
    "# Import Segement Data\n",
    "The segement data has the data type dictionaries. Therefore we need to request the data differenetly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dd9da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import certifi\n",
    "\n",
    "# Load the Excel file\n",
    "tickers_df = pd.read_excel(\"CompanyList_Coded.xlsx\")\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    with urlopen(url, cafile=certifi.where()) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "# Base URL and API key configuration\n",
    "base_url = \"https://financialmodelingprep.com/api/v4\"\n",
    "api_key = \"ieZWryBMhiEhowJQXvvJBSo8rcJfvMVi\"  # Replace 'YOUR_API_KEY' with your actual API key\n",
    "\n",
    "\n",
    "def fetch_ticker_data(tickers_df, base_url, datatype, api_key, endpoint_params=\"\"):\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for index, row in tickers_df.iterrows():\n",
    "        symbol = row['Ticker']\n",
    "        label = row['Label']\n",
    "        url = f\"{base_url}/{datatype}?symbol={symbol}&structure=flat{endpoint_params}&apikey={api_key}\"\n",
    "        try:\n",
    "            data = get_jsonparsed_data(url)\n",
    "            results.append({'Ticker': symbol, 'Label': label, 'Data': data})\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch data for {symbol}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "geo_revenue_df = fetch_ticker_data(tickers_df, base_url, \"revenue-geographic-segmentation\",api_key)\n",
    "product_revenue_df = fetch_ticker_data(tickers_df, base_url,'revenue-product-segmentation',api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_revenue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aggregate_category_counts(df):\n",
    "    # List to hold the aggregated data\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        Ticker = row['Ticker']\n",
    "        label = row['Label']\n",
    "        data_entries = row['Data']\n",
    "\n",
    "        if not data_entries:  # Check if the data_entries list is empty\n",
    "            # Generate rows for each year from 2023 to 2007\n",
    "            for year in range(2023, 2018, -1):\n",
    "                results.append({\n",
    "                    'Ticker': Ticker,\n",
    "                    'calendarYear': year,\n",
    "                    'Label': label,\n",
    "                    'Number of Product Segments': None\n",
    "                })\n",
    "        else:\n",
    "            # Process each year's data in the list\n",
    "            for entry in data_entries:\n",
    "                for year_date, revenues in entry.items():\n",
    "                    # Extract the year part from the date string\n",
    "                    year = pd.to_datetime(year_date).year\n",
    "\n",
    "                    # Count the number of unique product categories for the current year\n",
    "                    num_categories = len(revenues)\n",
    "\n",
    "                    # Append the result as a new row in the results list\n",
    "                    results.append({\n",
    "                        'Ticker': Ticker,\n",
    "                        'calendarYear': year,\n",
    "                        'Label': label,\n",
    "                        'Number of Product Segments': num_categories\n",
    "                    })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage assuming product_revenue_df and geo_revenue_df are defined\n",
    "product_revenue_df = aggregate_category_counts(product_revenue_df)\n",
    "geo_revenue_df = aggregate_category_counts(geo_revenue_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692af122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming 'Ticker' column to 'Symbol' in the DataFrame\n",
    "product_revenue_df = product_revenue_df.rename(columns={'Ticker': 'symbol'})\n",
    "geo_revenue_df = geo_revenue_df.rename(columns={'Ticker': 'symbol'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b31353",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_revenue_df = product_revenue_df[product_revenue_df['calendarYear'].between(2019, 2023)]\n",
    "geo_revenue_df = geo_revenue_df[geo_revenue_df['calendarYear'].between(2019,2023)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa5a1a",
   "metadata": {},
   "source": [
    "# Preparing the Data, Filtering Out only the necessary collumns\n",
    "\n",
    "After fetching the data, we filter out only the necessary columns from each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606038d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balancesheet_df = balancesheet_df[['calendarYear', 'symbol','totalCurrentAssets','totalNonCurrentAssets', \n",
    "                                   'totalAssets', 'totalCurrentLiabilities','totalNonCurrentLiabilities',\n",
    "                                   'totalLiabilities', 'Label', 'Partner']]\n",
    "\n",
    "\n",
    "incomestatement_df = incomestatement_df[['calendarYear', 'symbol','revenue', 'costOfRevenue', 'grossProfit',\n",
    "                                       'operatingExpenses', 'ebitda', 'Label']]\n",
    "\n",
    "cashflowstatement_df = cashflowstatement_df[['calendarYear', 'symbol', 'acquisitionsNet','investmentsInPropertyPlantAndEquipment',   \n",
    "                                            'commonStockIssued','debtRepayment','Label']]\n",
    "\n",
    "keymetrics_df = keymetrics_df[['calendarYear', 'symbol' ,\"debtToEquity\", 'debtToAssets', 'marketCap',                              \n",
    "                               'workingCapital', 'daysOfInventoryOnHand', 'Label']]\n",
    "\n",
    "employeecount_df = employeecount_df[['calendarYear', 'symbol', 'employeeCount', 'Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c182a12",
   "metadata": {},
   "source": [
    "## Balance Sheet Ratios \n",
    "\n",
    "- (Assets/Total Assets) * 100\n",
    "- (Liabilities/Total Liabilities) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_ratios(df, revenue_column='grossProfit'):\n",
    "    \n",
    "    # Create a copy of the original DataFrame to avoid modifying the original data\n",
    "    df_with_ratios = df.copy()\n",
    "    \n",
    "    # Calculate ratios for assets\n",
    "    asset_columns = ['totalCurrentAssets','totalNonCurrentAssets']\n",
    "    \n",
    "    total_assets = df_with_ratios[asset_columns].sum(axis=1)\n",
    "    \n",
    "    for feature_column in asset_columns:\n",
    "        df_with_ratios[f'{feature_column}_to_totalAssets_ratio'] = (df_with_ratios[feature_column] / total_assets) * 100\n",
    "    \n",
    "    # Calculate ratios for liabilities\n",
    "    liability_columns = ['totalCurrentLiabilities','totalNonCurrentLiabilities']\n",
    "    \n",
    "    total_liabilities = df_with_ratios[liability_columns].sum(axis=1)\n",
    "    \n",
    "    for feature_column in liability_columns:\n",
    "        df_with_ratios[f'{feature_column}_to_totalLiabilities_ratio'] = (df_with_ratios[feature_column] / total_liabilities) * 100\n",
    "    \n",
    "    # Drop the original features from the DataFrame\n",
    "    df_with_ratios.drop(columns=asset_columns + liability_columns, inplace=True)\n",
    "    \n",
    "    return df_with_ratios\n",
    "\n",
    "# Usage example:\n",
    "balancesheet_ratios_df = calculate_feature_ratios(balancesheet_df)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b61f6",
   "metadata": {},
   "source": [
    "# Cashflow / Income Statement Ratios\n",
    "\n",
    "- (Columns / Revenue) * 100 \n",
    "\n",
    "Used Columns\n",
    "- *incomestatement_columns* = ['costOfRevenue', 'grossProfit', 'researchAndDevelopmentExpenses', \n",
    "                               'sellingGeneralAndAdministrativeExpenses', 'operatingExpenses', \n",
    "                               'costAndExpenses', 'ebitda', 'operatingIncome']\n",
    "                               \n",
    "- *cashflow_columns* = ['netCashProvidedByOperatingActivities', 'netCashUsedForInvestingActivites',\n",
    "                        'investmentsInPropertyPlantAndEquipment', 'freeCashFlow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_feature_ratios(df, revenue_column='grossProfit', revenue_df=None):\n",
    "    # Verify input DataFrame and revenue DataFrame are provided\n",
    "    if df is None or revenue_df is None:\n",
    "        raise ValueError(\"Both 'df' and 'revenue_df' must be provided.\")\n",
    "    \n",
    "    # Verify the revenue column exists in revenue_df\n",
    "    if revenue_column not in revenue_df.columns:\n",
    "        raise ValueError(f\"The revenue column '{revenue_column}' was not found in the revenue DataFrame.\")\n",
    "    \n",
    "    # Create a copy of the original DataFrame to avoid modifying the original data\n",
    "    df_with_ratios = df.copy()\n",
    "    \n",
    "    # Define the columns for which to calculate ratios related to income statements\n",
    "    incomestatement_columns = ['revenue', 'costOfRevenue', 'grossProfit', 'operatingExpenses', 'ebitda',]\n",
    "    \n",
    "    # Calculate ratios for income statement features\n",
    "    for feature_column in incomestatement_columns:\n",
    "        if feature_column in df_with_ratios.columns:\n",
    "            df_with_ratios[f'{feature_column}_to_Revenue_ratio'] = (df_with_ratios[feature_column] / revenue_df[revenue_column]) * 100\n",
    "    \n",
    "    # Define the columns for cash flow calculation\n",
    "    cashflow_columns = ['acquisitionsNet','investmentsInPropertyPlantAndEquipment','commonStockIssued','debtRepayment',]\n",
    "    \n",
    "    # Calculate ratios for cash flow features\n",
    "    for feature_column in cashflow_columns:\n",
    "        if feature_column in df_with_ratios.columns:\n",
    "            df_with_ratios[f'{feature_column}_to_Revenue_ratio'] = (df_with_ratios[feature_column] / revenue_df[revenue_column]) * 100\n",
    "    \n",
    "    return df_with_ratios\n",
    "\n",
    "# Corrected example usage:\n",
    "cashflowstatement_ratios_df = calculate_feature_ratios(cashflowstatement_df, revenue_column='revenue', revenue_df=incomestatement_df)\n",
    "incomestatement_ratios_df = calculate_feature_ratios(incomestatement_df, revenue_column='revenue', revenue_df=incomestatement_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aea621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cashflowstatement_ratios_df = cashflowstatement_ratios_df.reset_index()\n",
    "incomestatement_ratios_df = incomestatement_ratios_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053a2d0",
   "metadata": {},
   "source": [
    "# Key Metrics Ratio \n",
    "\n",
    "- Here we just need the Ratio working capital / Revenue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e3633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keymetrics_df\n",
    "keymetrics_df[\"workingCapital_to_revenue_Ratio\"] = (keymetrics_df[\"workingCapital\"]/incomestatement_df[\"revenue\"])*100\n",
    "keymetrics_ratios_df = keymetrics_df.copy()\n",
    "keymetrics_ratios_df.drop(\"workingCapital\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a915875",
   "metadata": {},
   "source": [
    "# Employee Count / Revenue Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a535d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomestatement_df_red = incomestatement_df[[\"calendarYear\", \"symbol\", \"revenue\"]]\n",
    "employeecount_df[[\"calendarYear\", \"symbol\"]] = employeecount_df[[\"calendarYear\", \"symbol\"]].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec74440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming employeecount_df and incomestatement_df are your DataFrames\n",
    "\n",
    "# Convert calendarYear and symbol columns to string and trim any whitespaces\n",
    "for df in [employeecount_df, incomestatement_df]:\n",
    "    df['calendarYear'] = df['calendarYear'].astype(str).str.strip()\n",
    "    df['symbol'] = df['symbol'].astype(str).str.strip()\n",
    "\n",
    "# Filter the incomestatement_df to include only the necessary columns\n",
    "incomestatement_reduced = incomestatement_df[['calendarYear', 'symbol', 'revenue']]\n",
    "\n",
    "# Perform the merge using an inner join\n",
    "employeecount_ratios_df = pd.merge(employeecount_df, incomestatement_reduced, on=['calendarYear', 'symbol'], how='inner')\n",
    "employeecount_ratios_df[\"employee_revenue_ratio\"] = (employeecount_ratios_df[\"employeeCount\"]/employeecount_ratios_df[\"revenue\"])*100\n",
    "employeecount_ratios_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59328829",
   "metadata": {},
   "source": [
    "## Analyst Recommendations\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{WARS} = \\frac{(3 \\times \\text{analystRatingsStrongBuy}) + (2 \\times \\text{analystRatingsBuy}) + (\\text{analystRatingsHold}) - (2 \\times \\text{analystRatingsSell}) - (3 \\times \\text{analystRatingsStrongSell})}{\\text{Total Ratings}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Assign Weights to Each Category:\n",
    "Strong Buy: +3\n",
    "Buy: +2\n",
    "Hold: +1\n",
    "Sell: -2\n",
    "Strong Sell: -3\n",
    "\n",
    "\n",
    "- With this Ratio, we can evaluate the sentiment of analysts, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a70d4",
   "metadata": {},
   "source": [
    "## Partners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ddfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "balancesheet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de52c6",
   "metadata": {},
   "source": [
    "# Dropping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c6ae9",
   "metadata": {},
   "source": [
    "Cashflow-Statements Collumn Drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b15de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_2_drop = cashflowstatement_ratios_df[['acquisitionsNet','investmentsInPropertyPlantAndEquipment','commonStockIssued',\n",
    "                                        'debtRepayment']]\n",
    "cashflowstatement_ratios_df.drop(columns=cs_2_drop, axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_2_drop = incomestatement_ratios_df[[\"revenue\", 'costOfRevenue', 'grossProfit', 'operatingExpenses', 'ebitda' ]]\n",
    "incomestatement_ratios_df.drop(columns=is_2_drop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04849009",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_2_drop = balancesheet_df[[\"totalAssets\", \"totalLiabilities\"]]\n",
    "balancesheet_ratios_df.drop(columns=bs_2_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63847a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_2_drop = employeecount_ratios_df[[\"revenue\", \"employeeCount\"]]\n",
    "employeecount_ratios_df.drop(columns=ec_2_drop, axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385a245",
   "metadata": {},
   "source": [
    "# Final Financial Ratios\n",
    "\n",
    "- cashflowstatement_ratios_df\n",
    "- incomestatement_ratios_df\n",
    "- balancesheet_ratios_df\n",
    "- keymetrics_ratios_df\n",
    "- employeecount_ratios_df\n",
    "- product_revenue_df\n",
    "- geo_revenue_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd4f9e",
   "metadata": {},
   "source": [
    "## Imputing missing values using Regression\n",
    "- When we inspect the distinct dataframes, we can observe that they have different lengths, indicating that there are missing values.\n",
    "\n",
    "- However, to merge the dfs we need the same lenghts an no missing values, thus we use regression as an imputation method\n",
    "\n",
    "- The following function checks for every ticker if there is a missing value (year) creates and fills the row with the value obtained from the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a02112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def impute_missing_data(df):\n",
    "    if 'calendarYear' in df.columns:\n",
    "        df['calendarYear'] = pd.to_numeric(df['calendarYear'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Ensure DataFrame has unique symbol-year combinations\n",
    "    df = df.drop_duplicates(subset=['symbol', 'calendarYear'], keep='first')\n",
    "\n",
    "    # Get unique symbols and full year range\n",
    "    symbols = df['symbol'].unique()\n",
    "    all_years = range(2019, 2024)\n",
    "\n",
    "    # Create a full DataFrame with all symbols and years\n",
    "    full_df = pd.DataFrame([(s, y) for s in symbols for y in all_years], columns=['symbol', 'calendarYear'])\n",
    "\n",
    "    # Merge the original DataFrame with the full DataFrame to fill in missing years\n",
    "    merged_df = pd.merge(full_df, df, on=['symbol', 'calendarYear'], how='left')\n",
    "\n",
    "    # Forward fill and backward fill to populate missing 'Label' and other categorical or static data\n",
    "    merged_df['Label'] = merged_df.groupby('symbol')['Label'].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "    # Replace infinite values with NaN\n",
    "    merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Group by 'Label' and impute missing data for numeric columns using regression\n",
    "    for label in merged_df['Label'].dropna().unique():\n",
    "        label_group = merged_df[merged_df['Label'] == label]\n",
    "        numeric_cols = label_group.select_dtypes(include=['number']).columns.difference(['Label'])\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            not_null_data = label_group[label_group[col].notnull() & (label_group[col] != 0)]\n",
    "            null_data = label_group[label_group[col].isnull() | (label_group[col] == 0)]\n",
    "            \n",
    "            if not not_null_data.empty and not null_data.empty:\n",
    "                predictors = numeric_cols.drop(col)  # Use other numeric columns as predictors\n",
    "                valid_data = not_null_data.dropna(subset=predictors)\n",
    "                \n",
    "                if len(valid_data) > 0:\n",
    "                    reg = LinearRegression()\n",
    "                    reg.fit(valid_data[predictors], valid_data[col])\n",
    "\n",
    "                    # Predict missing or zero values if predictors are available\n",
    "                    predictors_null = null_data[predictors].dropna()\n",
    "                    if not predictors_null.empty:\n",
    "                        predicted_values = reg.predict(predictors_null)\n",
    "                        merged_df.loc[predictors_null.index, col] = predicted_values\n",
    "\n",
    "    # Finalize DataFrame by sorting and potentially filling any remaining missing numeric data with 0 or other methods\n",
    "    merged_df = merged_df.sort_values(by=['symbol', 'calendarYear'])\n",
    "    \n",
    "    # Optionally fill any remaining NaNs with 0 or other values\n",
    "    merged_df.fillna(0, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "balancesheet_ratios_df = impute_missing_data(balancesheet_ratios_df)\n",
    "cashflowstatement_ratios_df = impute_missing_data(cashflowstatement_ratios_df)\n",
    "incomestatement_ratios_df = impute_missing_data(incomestatement_ratios_df)\n",
    "keymetrics_ratios_df = impute_missing_data(keymetrics_ratios_df)\n",
    "employeecount_ratios_df = impute_missing_data(employeecount_ratios_df)\n",
    "product_revenue_df = impute_missing_data(product_revenue_df)\n",
    "geo_revenue_df = impute_missing_data(geo_revenue_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06127a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balancesheet_ratios_df.info()\n",
    "incomestatement_ratios_df.info()\n",
    "cashflowstatement_ratios_df.info()\n",
    "keymetrics_ratios_df.info()\n",
    "employeecount_ratios_df.info() \n",
    "geo_revenue_df.info()\n",
    "product_revenue_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2641be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df = balancesheet_ratios_df\n",
    "merged_df = pd.merge(merged_df, incomestatement_ratios_df, on=['symbol', 'calendarYear', 'Label'], how='outer')\n",
    "merged_df = pd.merge(merged_df, cashflowstatement_ratios_df, on=['symbol', 'calendarYear', 'Label' ], how='outer')\n",
    "merged_df = pd.merge(merged_df, keymetrics_ratios_df, on=['symbol', 'calendarYear', 'Label'], how='outer')\n",
    "merged_df = pd.merge(merged_df, employeecount_ratios_df, on=['symbol', 'calendarYear', 'Label'], how='outer')\n",
    "merged_df = pd.merge(merged_df, geo_revenue_df, on=['symbol', 'calendarYear', 'Label'], how='outer')\n",
    "merged_df = pd.merge(merged_df, product_revenue_df, on=['symbol', 'calendarYear', 'Label'], how='outer')\n",
    "\n",
    "# Check the merged DataFrame\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1298a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "impute_missing_data(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns={\"Number of Product Segments_y\": \"Number of Geo Segments\",\n",
    "                                      \"Number of Product Segments_x\":\"Number of Product Segments\" })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Number of Product Segments\"]= merged_df[\"Number of Product Segments\"].astype(\"float64\")\n",
    "merged_df[\"Number of Geo Segments\"]= merged_df[\"Number of Geo Segments\"].astype(\"float64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel(\"finaldf_partner_imp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58bd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
